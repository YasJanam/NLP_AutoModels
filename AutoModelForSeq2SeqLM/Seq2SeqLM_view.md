## โฌ Seq2SeqLM_Models
ูุฏูโูุง Seq2SeqLM (ูุฎูู Sequence-to-Sequence Language Models) ุฏุณุชูโุง ุงุฒ ูุนูุงุฑโูุง ุดุจฺฉูโูุง ุนุตุจ ูุณุชูุฏ ฺฉู ุจุฑุง ุญู ูุณุงุฆู ุทุฑุงุญ ุดุฏูโุงูุฏ ฺฉู ูุฑูุฏ ู ุฎุฑูุฌ ุขูโูุง ูุฑ ุฏู ุฏูุจุงููโุง ุงุฒ ุฏุงุฏูโูุง (ูุนูููุงู ูุชู) ูุณุชูุฏ.

### ๐ฅ ูุนุฑู ฺฉู

ุงุฏู ุงุตู ุฏุฑ ุงู ูุฏูโูุง ุงู ุงุณุช ฺฉู ฺฉ ุฏูุจุงูู ูุฑูุฏ (ูุงููุฏ ุฌูููโุง ุจู ฺฉ ุฒุจุงู) ฺฏุฑูุชู ูโุดูุฏ ู ุจู ฺฉ ุฏูุจุงูู ุฎุฑูุฌ (ูุงููุฏ ุชุฑุฌูู ููุงู ุฌููู ุจู ุฒุจุงู ุฏฺฏุฑ) ุชุจุฏู ูโฺฏุฑุฏุฏ. ุจุฑุง ุงู ฺฉุงุฑุ ูุนูุงุฑ ูุนูููุงู ุงุฒ ุฏู ุจุฎุด ุงุตู ุชุดฺฉู ูโุดูุฏ:

---
#### ๐ด Encoder (ุงูฺฉูุฏุฑ):
ูุฑูุฏ ุฑุง ุฏุฑุงูุช ฺฉุฑุฏู ู ุขู ุฑุง ุจู ููุงุด ุจุฑุฏุงุฑ ูุดุฑุฏู (context representation) ุชุจุฏู ูโฺฉูุฏ. ุงู ุจุฎุด ุณุงุฎุชุงุฑ ู ูุนูุง ุฏูุจุงูู ูุฑูุฏ ุฑุง ุงุณุชุฎุฑุงุฌ ูโฺฉูุฏ.

- __ูุธููโุงุด ฺฏุฑูุชู ุฏูุจุงููโ ูุฑูุฏ ู ุงุณุชุฎุฑุงุฌ ฺฉ ููุงุด ูุนูุง ูุดุฑุฏู ุงุฒ ุงูู ูุณุช.__

- ุฏุฑ ูุนูุงุฑโูุง ูุฏุฑูุ ุงู ุจุฎุด ุงุฒ ฺูุฏู ูุงู Self-Attention ุชุดฺฉู ุดุฏู ฺฉู ูุฑ ุชูฺฉู ูุฑูุฏ ุฑู ุฏุฑ ุฑุงุจุทู ุจุง ฺฉู ุฌููู ุฏุฑฺฉ ูโฺฉูู.

- ุฎุฑูุฌ ุงูฺฉูุฏุฑ: ุจุฑุฏุงุฑูุง ูุชู ฺฉู ุญุงู ุงุทูุงุนุงุช ุฒูููโุง ูุณุชู.

---  
#### ๐ด Decoder (ุฏฺฉูุฏุฑ):
ุจุง ุงุณุชูุงุฏู ุงุฒ ุจุฑุฏุงุฑ ูุชู ุงูฺฉูุฏุฑุ ุฏูุจุงูู ุฎุฑูุฌ ุฑุง ุจู ุตูุฑุช ูุฑุญููโุจูโูุฑุญูู (ุชูฺฉู ุจู ุชูฺฉู) ุชููุฏ ูโฺฉูุฏ. ุฏุฑ ูุฑ ูุฑุญููุ ุฏฺฉูุฏุฑ ุจู ุญุงูุชโูุง ูุจู ุฎูุฏ ู ุฎุฑูุฌ ุงูฺฉูุฏุฑ ุชูุฌู ูโฺฉูุฏ.

- __ูุธููโุงุด ุชููุฏ ุฏูุจุงููโ ุฎุฑูุฌ ุจู ุตูุฑุช ฺฏุงูโุจูโฺฏุงู ูุณุช.__

- ุฏุฑ ูุฑ ูุฑุญูู:

  - ุจู ุชูฺฉูโูุง ูุจู ุฎุฑูุฌ ูฺฏุงู ูโฺฉูู (ุงุฒ ุทุฑู Masked Self-Attention)

  - ุจู ุจุฑุฏุงุฑูุง ุงูฺฉูุฏุฑ ุชูุฌู ูโฺฉูู (Cross-Attention)

  - ฺฉ ุชูฺฉู ุฌุฏุฏ ุชููุฏ ูโฺฉูู.

- ุงู ุฑููุฏ ุชุง ุฑุณุฏู ุจู ูุดุงููโ ูพุงุงู ุฌููู (EOS) ุงุฏุงูู ูพุฏุง ูโฺฉูู.
---

### ูุงุจูุชโูุง ู ฺฉุงุฑุจุฑุฏูุง

ูุฏูโูุง Seq2SeqLM ุจู ุฎุงุทุฑ ุงูุนุทุงู ู ูุฏุฑุช ุฏุฑ ุฏุฑฺฉ ู ุชููุฏ ุฒุจุงู ุทุจุนุ ุฏุฑ ุจุณุงุฑ ุงุฒ ูุธุงู NLP ุงุณุชูุงุฏู ูโุดููุฏุ ุงุฒ ุฌููู:

ุชุฑุฌูู ูุงุดู (ูุซูุงู ุชุจุฏู ุงูฺฏูุณ ุจู ูุงุฑุณ)

ุฎูุงุตูโุณุงุฒ ูุชูู

ูพุงุณุฎ ุจู ูพุฑุณุดโูุง

ุชููุฏ ูุชู ุงุฏุงููโุฏุงุฑ

ฺฏูุชฺฏู ู ฺุชโุจุงุชโูุง


### ๐ ูุฏูโูุง ูุดููุฑ Seq2SeqLM

- Seq2Seq RNN (LSTM/GRU) โ ูุนูุงุฑ ุงููู (ูุซู Google Translate ูุฏู)

- Transformer-based (ูุฏุฑูโุชุฑ):

  - BART โ ุฎูุงุตูโุณุงุฒ ู ุจุงุฒุณุงุฒ ูุชู

  - T5 โ ูุฏู ูููโฺฉุงุฑู "Text-to-Text"

  - mBART โ ูุณุฎู ฺูุฏุฒุจุงูู BART

  - MarianMT โ ุชุฑุฌูู ูุงุดู ฺูุฏุฒุจุงูู
 
---

### BART vs MarianMT vs T5 vs Pegasus

|  Feature  | BART | MarianMT | T5 | Pegasus | 
|:--------:|:--------:|:--------:|:--------:|:--------:|
| Architecture | Transformer Seq2Seq ( denoising autoencoder ) | Transformer Seq2Seq ( encoder-decoder ) | Transformer Seq2Seq ( text-to-text ) | Transformer Seq2Seq ( summarization-focused ) |
| main-application | multi-task : translation,summarization,text-generation,QA | Machine-Translation | all NLP text-to-text Tasks | abstractive summarization |
| pretraining-Method | __Denoising autoencoder__ (masking, shuffling, deletion) | __only on translation data__ | __span-masked denoising__ on C4 | __Gap-Sentence Generation__ ( ุญุฐู ุฌููุงุช ฺฉูุฏ ) |
| ุงูุนุทุงู ูพุฐุฑ | ุจุงูุง( ฺฉุงุฑุจุฑุฏ ุฏุฑ ุจุณุงุฑ ุงุฒ ูุธุงู ูุชู ุจู ูุชู) | ูุญุฏูุฏ ุจู ุชุฑุฌูู | ุจุณุงุฑ ุจุงูุง | ูุญุฏูุฏุชุฑุูู ููู ุงูุนุงุฏู ูู ุฏุฑ ุฎูุงุตู ุณุงุฒ |
| ูุฒุช ุงุตู | ููู ฺฉุงุฑูุ ุนููฺฉุฑุฏ ุฎูุจ ุฏุฑ ฺูุฏู ุชุณฺฉ |ุณุจฺฉ ู ุณุฑุน ุจุฑุง ุชุฑุฌูู | ฺฉูพุงุฑฺฺฏ ููู ูุธุงู nlp ุฏุฑ ูุงูุจ ูุชู ุจู ูุชู | ุจูุชุฑู ุนููฺฉุฑุฏ ุฏุฑ ุฎูุงุตู ุณุงุฒ |
| ูุนุงุจ | ูุณุจุชุง ุณูฺฏู | ููุท ุชุฑุฌููุูู ฺฉุงุฑูุง ุฏฺฏู | ุจุณุงุฑ ุจุฒุฑฺฏ | ุชุฎุตุต ุจูุฏู |



---
### denoising autoencoder 
ุงู ุฑูุด ฺฉ ุฑูุด ูพุด ุขููุฒุด ุงุณุช ฺฉู ุฏุฑ ูุฏู ูุง ุจุงุฑุช ุงุณุชูุงุฏู ูุดูุฏ. ุจุฑุง ูุดุงูุฏู ฺฉ ููููู ูพุงุฏู ุณุงุฒ ุงู ุฑูุด ุจู ููฺฉ ุฒุฑ ุจุฑูุฏ :
https://github.com/YasJanam/NLP_MODELS_1/tree/main/Labratory_12

ุจุฑุง ูุดุงูุฏู ุชูุถุญุงุช ูุฑูุจุท ุจู ุชูุงุจุน ููุฑุฏ ุงุณุชูุงุฏู ุฏุฑ ูพุด ุขููุฒุด ููฺฉ ุจุงูุง ุจู ููฺฉ ุฒุฑ ุจุฑูุฏ :
https://github.com/YasJanam/NLP_MODELS_1/tree/main/Pretrain_Methods_11

---


