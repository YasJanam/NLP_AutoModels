## Seq2SeqLM_Models
مدل‌های Seq2SeqLM (مخفف Sequence-to-Sequence Language Models) دسته‌ای از معماری‌های شبکه‌های عصبی هستند که برای حل مسائلی طراحی شده‌اند که ورودی و خروجی آن‌ها هر دو دنباله‌ای از داده‌ها (معمولاً متن) هستند.

### 🟥 معرفی کلی

ایده اصلی در این مدل‌ها این است که یک دنباله ورودی (مانند جمله‌ای به یک زبان) گرفته می‌شود و به یک دنباله خروجی (مانند ترجمه همان جمله به زبان دیگر) تبدیل می‌گردد. برای این کار، معماری معمولاً از دو بخش اصلی تشکیل می‌شود:

---
#### 🔴 Encoder (انکودر):
ورودی را دریافت کرده و آن را به نمایش برداری فشرده (context representation) تبدیل می‌کند. این بخش ساختار و معنای دنباله ورودی را استخراج می‌کند.

- __وظیفه‌اش گرفتن دنباله‌ی ورودی و استخراج یک نمایش معنایی فشرده از اون هست.__

- در معماری‌های مدرن، این بخش از چندین لایه Self-Attention تشکیل شده که هر توکن ورودی رو در رابطه با کل جمله درک می‌کنه.

- خروجی انکودر: بردارهای متنی که حاوی اطلاعات زمینه‌ای هستن.

---  
#### 🔴 Decoder (دیکودر):
با استفاده از بردار متنی انکودر، دنباله خروجی را به صورت مرحله‌به‌مرحله (توکن به توکن) تولید می‌کند. در هر مرحله، دیکودر به حالت‌های قبلی خود و خروجی انکودر توجه می‌کند.

- __وظیفه‌اش تولید دنباله‌ی خروجی به صورت گام‌به‌گام هست.__

- در هر مرحله:

  - به توکن‌های قبلی خروجی نگاه می‌کنه (از طریق Masked Self-Attention)

  - به بردارهای انکودر توجه می‌کنه (Cross-Attention)

  - یک توکن جدید تولید می‌کنه.

- این روند تا رسیدن به نشانه‌ی پایان جمله (EOS) ادامه پیدا می‌کنه.
---

### قابلیت‌ها و کاربردها

مدل‌های Seq2SeqLM به خاطر انعطاف و قدرت در درک و تولید زبان طبیعی، در بسیاری از وظایف NLP استفاده می‌شوند، از جمله:

ترجمه ماشینی (مثلاً تبدیل انگلیسی به فارسی)

خلاصه‌سازی متون

پاسخ به پرسش‌ها

تولید متن ادامه‌دار

گفتگو و چت‌بات‌ها


### 🔑 مدل‌های مشهور Seq2SeqLM

- Seq2Seq RNN (LSTM/GRU) → معماری اولیه (مثل Google Translate قدیمی)

- Transformer-based (مدرن‌تر):

 - BART → خلاصه‌سازی و بازسازی متن

 - T5 → مدل همه‌کاره "Text-to-Text"

 - mBART → نسخه چندزبانه BART

 - MarianMT → ترجمه ماشینی چندزبانه
