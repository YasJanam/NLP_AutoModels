{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jSoVeW5zNDEE"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "1843d68b00db4a1e934da5fbd2065b58",
      "b16bf8d3f2474754902f0bb4f4d74318",
      "2d3bc0fb432e4fb4a450013b61521f6c",
      "5d7715e09a8848768826f135f39734a2",
      "6d89e11b90a84b38a1ba7d5123b68176",
      "c16e7c3c814f48f8ab8783c4902aeb4c",
      "22335ce31e964b50af5d4c68826ea2cc",
      "5386b0e4eecb407f930840715c90c26c",
      "f8e120e868c449f682bfda1a7c1548b6",
      "2cbd50ce62f448d286753ed09d5d1a63",
      "65a97d4ab9ae4ea48346edd1c8fe7e42",
      "ec6e3b2bce564548afc15bb7090d22c9",
      "844d129ddc4048809ffe9a7292b1e32c",
      "13333288bd864abebeacfd1ccac40e1d",
      "7cc96c64b4174d84a6880aa2e58ca6ef",
      "79bf99309d3941b48868bb04ceebe975",
      "661272b899aa4b7a888eeea234705bb9",
      "7da2fe95b1ab4bce8a82b2729164276e",
      "c205d10e175745c18b70ac364c310cab",
      "dcb447e944a04fbab57f78093cdc3096",
      "295c794ea0e347638cb0a1eb22b47bd0",
      "325a12e55d984df5ba000bb042b4fdb1",
      "d9a5131fe2744696ac09ece3d903732d",
      "934d076902ca4083b549e9827170c0dd",
      "251051a3371c46639f2b4b77ab68f06d",
      "cc22837d3de74972bae0867a0773c0fc",
      "f96a5418469d485b9840882cfc348e12",
      "e547f8309acd4f7db4646904d7fc24ee",
      "e11a9f4ed4c349dcb288a4d694543afb",
      "43065f672e614c9ab9bcd785cdcf6f54",
      "a1478a748859416d9cd4d1b7507f9849",
      "79eb2309903d4d89830b616a6c75d117",
      "89e62475049c44aa9f07343f13b154c6"
     ]
    },
    "id": "N3JGgUyxNS8A",
    "outputId": "363f3507-f9ec-483c-cd8e-3182d176009d"
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-large-cnn\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DNmEF6DCSCFD"
   },
   "source": [
    "#### ðŸŸ¢ **Decoder-Layers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "W_ZZJt2jSCFD",
    "outputId": "039e5315-0b99-4196-846d-dc20d1ba8239"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "decoder layers : \n",
      "layer 1 : BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "layer 2 : BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "layer 3 : BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "layer 4 : BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "layer 5 : BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "layer 6 : BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "layer 7 : BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "layer 8 : BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "layer 9 : BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "layer 10 : BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "layer 11 : BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "layer 12 : BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(\"decoder layers : \")\n",
    "for i, layer in enumerate(model.model.decoder.layers):\n",
    "  print(f\"layer {i+1} : {layer}\")"
   ]
  }
 ],
 "metadata": {
  "cells": [],
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  },
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "name": "python",
    "version": "3.10.12"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
