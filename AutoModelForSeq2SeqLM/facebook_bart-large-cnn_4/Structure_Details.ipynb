{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jSoVeW5zNDEE"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 237,
     "referenced_widgets": [
      "1843d68b00db4a1e934da5fbd2065b58",
      "b16bf8d3f2474754902f0bb4f4d74318",
      "2d3bc0fb432e4fb4a450013b61521f6c",
      "5d7715e09a8848768826f135f39734a2",
      "6d89e11b90a84b38a1ba7d5123b68176",
      "c16e7c3c814f48f8ab8783c4902aeb4c",
      "22335ce31e964b50af5d4c68826ea2cc",
      "5386b0e4eecb407f930840715c90c26c",
      "f8e120e868c449f682bfda1a7c1548b6",
      "2cbd50ce62f448d286753ed09d5d1a63",
      "65a97d4ab9ae4ea48346edd1c8fe7e42",
      "ec6e3b2bce564548afc15bb7090d22c9",
      "844d129ddc4048809ffe9a7292b1e32c",
      "13333288bd864abebeacfd1ccac40e1d",
      "7cc96c64b4174d84a6880aa2e58ca6ef",
      "79bf99309d3941b48868bb04ceebe975",
      "661272b899aa4b7a888eeea234705bb9",
      "7da2fe95b1ab4bce8a82b2729164276e",
      "c205d10e175745c18b70ac364c310cab",
      "dcb447e944a04fbab57f78093cdc3096",
      "295c794ea0e347638cb0a1eb22b47bd0",
      "325a12e55d984df5ba000bb042b4fdb1",
      "d9a5131fe2744696ac09ece3d903732d",
      "934d076902ca4083b549e9827170c0dd",
      "251051a3371c46639f2b4b77ab68f06d",
      "cc22837d3de74972bae0867a0773c0fc",
      "f96a5418469d485b9840882cfc348e12",
      "e547f8309acd4f7db4646904d7fc24ee",
      "e11a9f4ed4c349dcb288a4d694543afb",
      "43065f672e614c9ab9bcd785cdcf6f54",
      "a1478a748859416d9cd4d1b7507f9849",
      "79eb2309903d4d89830b616a6c75d117",
      "89e62475049c44aa9f07343f13b154c6"
     ]
    },
    "id": "N3JGgUyxNS8A",
    "outputId": "363f3507-f9ec-483c-cd8e-3182d176009d"
   },
   "outputs": [],
   "source": [
    "model_name = \"facebook/bart-large-cnn\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCaBiKCocPlu"
   },
   "source": [
    "#### ðŸŸ  **Ø³Ø§Ø®ØªØ§Ø± Ù…Ø¯Ù„ Ø¨Ø§ Ø¬Ø²ÛŒÛŒØ§Øª (Structure_Details)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "spDPddggS6Ui",
    "outputId": "e027c9eb-16b4-40c4-d87b-57f831d8b13e",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " BartForConditionalGeneration(\n",
      "  (model): BartModel(\n",
      "    (shared): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "    (encoder): BartEncoder(\n",
      "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x BartEncoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (activation_fn): GELUActivation()\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "    (decoder): BartDecoder(\n",
      "      (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "      (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x BartDecoderLayer(\n",
      "          (self_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (activation_fn): GELUActivation()\n",
      "          (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (encoder_attn): BartAttention(\n",
      "            (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "            (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          )\n",
      "          (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "          (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "          (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1024, out_features=50264, bias=False)\n",
      ")\n",
      "model BartModel(\n",
      "  (shared): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "  (encoder): BartEncoder(\n",
      "    (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x BartEncoderLayer(\n",
      "        (self_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (activation_fn): GELUActivation()\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): BartDecoder(\n",
      "    (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "    (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "    (layers): ModuleList(\n",
      "      (0-11): 12 x BartDecoderLayer(\n",
      "        (self_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (activation_fn): GELUActivation()\n",
      "        (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (encoder_attn): BartAttention(\n",
      "          (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "          (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        )\n",
      "        (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "        (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "        (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "        (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      )\n",
      "    )\n",
      "    (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "model.shared BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "model.encoder BartEncoder(\n",
      "  (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "  (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x BartEncoderLayer(\n",
      "      (self_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (activation_fn): GELUActivation()\n",
      "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.embed_tokens BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "model.encoder.embed_positions BartLearnedPositionalEmbedding(1026, 1024)\n",
      "model.encoder.layers ModuleList(\n",
      "  (0-11): 12 x BartEncoderLayer(\n",
      "    (self_attn): BartAttention(\n",
      "      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (activation_fn): GELUActivation()\n",
      "    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "model.encoder.layers.0 BartEncoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.0.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.0.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.0.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.0.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.0.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.0.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.0.activation_fn GELUActivation()\n",
      "model.encoder.layers.0.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.0.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.0.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.1 BartEncoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.1.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.1.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.1.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.1.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.1.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.1.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.1.activation_fn GELUActivation()\n",
      "model.encoder.layers.1.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.1.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.1.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.2 BartEncoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.2.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.2.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.2.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.2.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.2.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.2.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.2.activation_fn GELUActivation()\n",
      "model.encoder.layers.2.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.2.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.2.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.3 BartEncoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.3.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.3.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.3.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.3.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.3.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.3.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.3.activation_fn GELUActivation()\n",
      "model.encoder.layers.3.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.3.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.3.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.4 BartEncoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.4.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.4.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.4.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.4.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.4.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.4.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.4.activation_fn GELUActivation()\n",
      "model.encoder.layers.4.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.4.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.4.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.5 BartEncoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.5.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.5.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.5.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.5.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.5.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.5.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.5.activation_fn GELUActivation()\n",
      "model.encoder.layers.5.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.5.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.5.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.6 BartEncoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.6.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.6.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.6.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.6.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.6.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.6.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.6.activation_fn GELUActivation()\n",
      "model.encoder.layers.6.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.6.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.6.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.7 BartEncoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.7.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.7.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.7.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.7.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.7.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.7.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.7.activation_fn GELUActivation()\n",
      "model.encoder.layers.7.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.7.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.7.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.8 BartEncoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.8.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.8.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.8.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.8.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.8.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.8.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.8.activation_fn GELUActivation()\n",
      "model.encoder.layers.8.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.8.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.8.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.9 BartEncoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.9.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.9.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.9.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.9.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.9.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.9.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.9.activation_fn GELUActivation()\n",
      "model.encoder.layers.9.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.9.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.9.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.10 BartEncoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.10.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.10.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.10.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.10.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.10.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.10.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.10.activation_fn GELUActivation()\n",
      "model.encoder.layers.10.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.10.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.10.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.11 BartEncoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (activation_fn): GELUActivation()\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.encoder.layers.11.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.encoder.layers.11.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.11.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.11.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.11.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.encoder.layers.11.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layers.11.activation_fn GELUActivation()\n",
      "model.encoder.layers.11.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.encoder.layers.11.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.encoder.layers.11.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.encoder.layernorm_embedding LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder BartDecoder(\n",
      "  (embed_tokens): BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "  (embed_positions): BartLearnedPositionalEmbedding(1026, 1024)\n",
      "  (layers): ModuleList(\n",
      "    (0-11): 12 x BartDecoderLayer(\n",
      "      (self_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (activation_fn): GELUActivation()\n",
      "      (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (encoder_attn): BartAttention(\n",
      "        (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "        (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      )\n",
      "      (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "      (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "      (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "      (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.embed_tokens BartScaledWordEmbedding(50264, 1024, padding_idx=1)\n",
      "model.decoder.embed_positions BartLearnedPositionalEmbedding(1026, 1024)\n",
      "model.decoder.layers ModuleList(\n",
      "  (0-11): 12 x BartDecoderLayer(\n",
      "    (self_attn): BartAttention(\n",
      "      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (activation_fn): GELUActivation()\n",
      "    (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder_attn): BartAttention(\n",
      "      (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "      (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    )\n",
      "    (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "    (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "    (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "    (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      ")\n",
      "model.decoder.layers.0 BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.0.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.0.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.activation_fn GELUActivation()\n",
      "model.decoder.layers.0.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.0.encoder_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.0.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.0.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.0.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.0.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.1 BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.1.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.1.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.activation_fn GELUActivation()\n",
      "model.decoder.layers.1.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.1.encoder_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.1.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.1.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.1.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.1.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.2 BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.2.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.2.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.activation_fn GELUActivation()\n",
      "model.decoder.layers.2.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.2.encoder_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.2.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.2.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.2.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.2.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.3 BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.3.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.3.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.activation_fn GELUActivation()\n",
      "model.decoder.layers.3.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.3.encoder_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.3.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.3.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.3.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.3.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.4 BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.4.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.4.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.activation_fn GELUActivation()\n",
      "model.decoder.layers.4.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.4.encoder_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.4.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.4.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.4.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.4.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.5 BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.5.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.5.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.activation_fn GELUActivation()\n",
      "model.decoder.layers.5.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.5.encoder_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.5.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.5.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.5.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.5.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.6 BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.6.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.6.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.activation_fn GELUActivation()\n",
      "model.decoder.layers.6.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.6.encoder_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.6.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.6.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.6.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.6.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.7 BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.7.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.7.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.activation_fn GELUActivation()\n",
      "model.decoder.layers.7.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.7.encoder_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.7.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.7.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.7.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.7.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.8 BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.8.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.8.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.activation_fn GELUActivation()\n",
      "model.decoder.layers.8.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.8.encoder_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.8.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.8.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.8.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.8.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.9 BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.9.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.9.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.activation_fn GELUActivation()\n",
      "model.decoder.layers.9.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.9.encoder_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.9.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.9.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.9.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.9.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.10 BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.10.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.10.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.activation_fn GELUActivation()\n",
      "model.decoder.layers.10.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.10.encoder_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.10.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.10.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.10.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.10.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.11 BartDecoderLayer(\n",
      "  (self_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (activation_fn): GELUActivation()\n",
      "  (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (encoder_attn): BartAttention(\n",
      "    (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "    (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  )\n",
      "  (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "  (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
      "  (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
      "  (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      ")\n",
      "model.decoder.layers.11.self_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.11.self_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.self_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.self_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.self_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.activation_fn GELUActivation()\n",
      "model.decoder.layers.11.self_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.11.encoder_attn BartAttention(\n",
      "  (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      "  (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
      ")\n",
      "model.decoder.layers.11.encoder_attn.k_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.encoder_attn.v_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.encoder_attn.q_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.encoder_attn.out_proj Linear(in_features=1024, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.encoder_attn_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layers.11.fc1 Linear(in_features=1024, out_features=4096, bias=True)\n",
      "model.decoder.layers.11.fc2 Linear(in_features=4096, out_features=1024, bias=True)\n",
      "model.decoder.layers.11.final_layer_norm LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "model.decoder.layernorm_embedding LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
      "lm_head Linear(in_features=1024, out_features=50264, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for name, module in model.named_modules():\n",
    "  print(name,module)"
   ]
  }
 ],
 "metadata": {
  "cells": [],
  "kernelspec": {
   "display_name": "",
   "name": ""
  },
  "language_info": {
   "name": ""
  },
  "metadata": {
   "kernelspec": {
    "display_name": "Python 3",
    "language": "python",
    "name": "python3"
   },
   "language_info": {
    "name": "python",
    "version": "3.10.12"
   }
  },
  "nbformat": 4,
  "nbformat_minor": 5
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
