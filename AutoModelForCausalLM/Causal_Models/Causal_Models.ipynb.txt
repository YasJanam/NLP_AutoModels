{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "0q6Wl7pgAsnA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. GPT-2**"
      ],
      "metadata": {
        "id": "sCSMYzlmAgzi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYaLIGBUYknj",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"iran in year 1403\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n\\ngenerated_text : \", generated_text)"
      ],
      "metadata": {
        "id": "CKbeHjjMBdUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo4m2o_hCotI",
        "outputId": "449c9926-c947-46ae-da15-5174d8f9cde7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### طبق خروجی بالا gpt2 ویژگی های\n",
        "\n",
        "vocabulary-size = 50257\n",
        "\n",
        "GPT2Block ساختار مدل ⟸ 12 بلاک\n",
        "\n",
        "میدانیم که خروجی مدل های علی در نهایت یک بردار به اندازه تعداد واژگان است. علتش هم این بود که خروجی مدل های علی یک توزیع احتمال روی واژگان دیکشنری است. برای اینکه خروجی به این شکل باشد، اخر شبکه مدل علی همیشه یک لایه خطی میگذارند که خروجی را به برداری با اندازه تعداد واژگان تبدیل کند. این لایه ای که راجبش صحبت میکنیم، لایه زیر است (اخرین لایه در بالا) :    \n",
        "\n",
        "🟥 **(lm_head): Linear(in_features=768, out_features=50257, bias=False)**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uD9L5yYUDuuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. GPT-Neo**"
      ],
      "metadata": {
        "id": "v6s_Zi4jHb7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"EleutherAI/gpt-neo-125M\"  # نسخه سبک‌ تر\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",          # خودش روی GPU/CPU پخش می‌کنه\n",
        "    torch_dtype=\"auto\",         # حافظه کمتر\n",
        ")"
      ],
      "metadata": {
        "id": "V7UKZ-uLHi_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"a cute cat\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n\\ngenerated_text : \", generated_text)"
      ],
      "metadata": {
        "id": "0SYo5ELZHqBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go8T01mdHsru",
        "outputId": "b519b82c-8387-4971-d537-dc267f914206"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPTNeoForCausalLM(\n",
            "  (transformer): GPTNeoModel(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(2048, 768)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. XLNet**"
      ],
      "metadata": {
        "id": "FRBkbHjgKiLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "استفاده میکند Permutation Language Modeling این مدل از روش"
      ],
      "metadata": {
        "id": "8Qf58AUeO-N_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"xlnet-base-cased\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GIhCl2c4KiLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"technology in iran\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n\\ngenerated_text : \", generated_text)"
      ],
      "metadata": {
        "id": "K3wv1p2gKiLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YBIEujjKiLS",
        "outputId": "e4c47384-eb03-4d0b-83d4-a2865193949e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XLNetLMHeadModel(\n",
            "  (transformer): XLNetModel(\n",
            "    (word_embedding): Embedding(32000, 768)\n",
            "    (layer): ModuleList(\n",
            "      (0-11): 12 x XLNetLayer(\n",
            "        (rel_attn): XLNetRelativeAttention(\n",
            "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ff): XLNetFeedForward(\n",
            "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation_function): GELUActivation()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lm_loss): Linear(in_features=768, out_features=32000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **معماری مدل**\n",
        "\n",
        "است XLNetLayer کل معماری این مدل 12 لایه\n",
        "\n",
        "هر لایه این مدل یک مکانیزم توجه + شبکه پیش خور است\n",
        "\n",
        "XLNetLayer : RelativeAttention + FeedForward\n",
        "\n",
        "vocab_size : 32000\n",
        "\n",
        "طول بردار خروجی این مدل : 32000   \n",
        "\n",
        "آخرین لایه خطی این مدل که بردار های خروجی تولید میکند :    \n",
        "\n",
        "🟧 **(lm_loss): Linear(in_features=768, out_features=32000, bias=True)**\n"
      ],
      "metadata": {
        "id": "LK44vYnKLtA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. CTRL**"
      ],
      "metadata": {
        "id": "IPkjz38eOaQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Salesforce/ctrl\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,force_download=True)"
      ],
      "metadata": {
        "id": "tkKJKj1NOaQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"a little story\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n\\ngenerated_text : \", generated_text)"
      ],
      "metadata": {
        "id": "K6oJT7OKOaQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "jJS-mLMQOaQz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}