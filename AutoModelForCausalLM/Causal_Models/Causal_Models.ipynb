{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer"
      ],
      "metadata": {
        "id": "0q6Wl7pgAsnA"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. GPT-2**"
      ],
      "metadata": {
        "id": "sCSMYzlmAgzi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYaLIGBUYknj",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "model_name = \"gpt2\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"iran in year 1403\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n\\ngenerated_text : \", generated_text)"
      ],
      "metadata": {
        "id": "CKbeHjjMBdUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yo4m2o_hCotI",
        "outputId": "449c9926-c947-46ae-da15-5174d8f9cde7"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Ø·Ø¨Ù‚ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ø§Ù„Ø§ gpt2 ÙˆÛŒÚ˜Ú¯ÛŒ Ù‡Ø§ÛŒ\n",
        "\n",
        "vocabulary-size = 50257\n",
        "\n",
        "GPT2Block Ø³Ø§Ø®ØªØ§Ø± Ù…Ø¯Ù„ âŸ¸ 12 Ø¨Ù„Ø§Ú©\n",
        "\n",
        "Ù…ÛŒØ¯Ø§Ù†ÛŒÙ… Ú©Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ Ù‡Ø§ÛŒ Ø¹Ù„ÛŒ Ø¯Ø± Ù†Ù‡Ø§ÛŒØª ÛŒÚ© Ø¨Ø±Ø¯Ø§Ø± Ø¨Ù‡ Ø§Ù†Ø¯Ø§Ø²Ù‡ ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø§Ø³Øª. Ø¹Ù„ØªØ´ Ù‡Ù… Ø§ÛŒÙ† Ø¨ÙˆØ¯ Ú©Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ù…Ø¯Ù„ Ù‡Ø§ÛŒ Ø¹Ù„ÛŒ ÛŒÚ© ØªÙˆØ²ÛŒØ¹ Ø§Ø­ØªÙ…Ø§Ù„ Ø±ÙˆÛŒ ÙˆØ§Ú˜Ú¯Ø§Ù† Ø¯ÛŒÚ©Ø´Ù†Ø±ÛŒ Ø§Ø³Øª. Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ú©Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ø¨Ù‡ Ø§ÛŒÙ† Ø´Ú©Ù„ Ø¨Ø§Ø´Ø¯ØŒ Ø§Ø®Ø± Ø´Ø¨Ú©Ù‡ Ù…Ø¯Ù„ Ø¹Ù„ÛŒ Ù‡Ù…ÛŒØ´Ù‡ ÛŒÚ© Ù„Ø§ÛŒÙ‡ Ø®Ø·ÛŒ Ù…ÛŒÚ¯Ø°Ø§Ø±Ù†Ø¯ Ú©Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ø±Ø§ Ø¨Ù‡ Ø¨Ø±Ø¯Ø§Ø±ÛŒ Ø¨Ø§ Ø§Ù†Ø¯Ø§Ø²Ù‡ ØªØ¹Ø¯Ø§Ø¯ ÙˆØ§Ú˜Ú¯Ø§Ù† ØªØ¨Ø¯ÛŒÙ„ Ú©Ù†Ø¯. Ø§ÛŒÙ† Ù„Ø§ÛŒÙ‡ Ø§ÛŒ Ú©Ù‡ Ø±Ø§Ø¬Ø¨Ø´ ØµØ­Ø¨Øª Ù…ÛŒÚ©Ù†ÛŒÙ…ØŒ Ù„Ø§ÛŒÙ‡ Ø²ÛŒØ± Ø§Ø³Øª (Ø§Ø®Ø±ÛŒÙ† Ù„Ø§ÛŒÙ‡ Ø¯Ø± Ø¨Ø§Ù„Ø§) :    \n",
        "\n",
        "ğŸŸ¥ **(lm_head): Linear(in_features=768, out_features=50257, bias=False)**\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uD9L5yYUDuuX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. GPT-Neo**"
      ],
      "metadata": {
        "id": "v6s_Zi4jHb7E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"EleutherAI/gpt-neo-125M\"  # Ù†Ø³Ø®Ù‡ Ø³Ø¨Ú©â€Œ ØªØ±\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\",          # Ø®ÙˆØ¯Ø´ Ø±ÙˆÛŒ GPU/CPU Ù¾Ø®Ø´ Ù…ÛŒâ€ŒÚ©Ù†Ù‡\n",
        "    torch_dtype=\"auto\",         # Ø­Ø§ÙØ¸Ù‡ Ú©Ù…ØªØ±\n",
        ")"
      ],
      "metadata": {
        "id": "V7UKZ-uLHi_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"a cute cat\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n\\ngenerated_text : \", generated_text)"
      ],
      "metadata": {
        "id": "0SYo5ELZHqBc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go8T01mdHsru",
        "outputId": "b519b82c-8387-4971-d537-dc267f914206"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPTNeoForCausalLM(\n",
            "  (transformer): GPTNeoModel(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(2048, 768)\n",
            "    (drop): Dropout(p=0.0, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-11): 12 x GPTNeoBlock(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPTNeoAttention(\n",
            "          (attention): GPTNeoSelfAttention(\n",
            "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
            "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
            "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "          )\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPTNeoMLP(\n",
            "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. XLNet**"
      ],
      "metadata": {
        "id": "FRBkbHjgKiLQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ø§Ø³ØªÙØ§Ø¯Ù‡ Ù…ÛŒÚ©Ù†Ø¯ Permutation Language Modeling Ø§ÛŒÙ† Ù…Ø¯Ù„ Ø§Ø² Ø±ÙˆØ´"
      ],
      "metadata": {
        "id": "8Qf58AUeO-N_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"xlnet-base-cased\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GIhCl2c4KiLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"technology in iran\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n\\ngenerated_text : \", generated_text)"
      ],
      "metadata": {
        "id": "K3wv1p2gKiLS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8YBIEujjKiLS",
        "outputId": "e4c47384-eb03-4d0b-83d4-a2865193949e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "XLNetLMHeadModel(\n",
            "  (transformer): XLNetModel(\n",
            "    (word_embedding): Embedding(32000, 768)\n",
            "    (layer): ModuleList(\n",
            "      (0-11): 12 x XLNetLayer(\n",
            "        (rel_attn): XLNetRelativeAttention(\n",
            "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ff): XLNetFeedForward(\n",
            "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation_function): GELUActivation()\n",
            "        )\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (lm_loss): Linear(in_features=768, out_features=32000, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Ù…Ø¹Ù…Ø§Ø±ÛŒ Ù…Ø¯Ù„**\n",
        "\n",
        "Ø§Ø³Øª XLNetLayer Ú©Ù„ Ù…Ø¹Ù…Ø§Ø±ÛŒ Ø§ÛŒÙ† Ù…Ø¯Ù„ 12 Ù„Ø§ÛŒÙ‡\n",
        "\n",
        "Ù‡Ø± Ù„Ø§ÛŒÙ‡ Ø§ÛŒÙ† Ù…Ø¯Ù„ ÛŒÚ© Ù…Ú©Ø§Ù†ÛŒØ²Ù… ØªÙˆØ¬Ù‡ + Ø´Ø¨Ú©Ù‡ Ù¾ÛŒØ´ Ø®ÙˆØ± Ø§Ø³Øª\n",
        "\n",
        "XLNetLayer : RelativeAttention + FeedForward\n",
        "\n",
        "vocab_size : 32000\n",
        "\n",
        "Ø·ÙˆÙ„ Ø¨Ø±Ø¯Ø§Ø± Ø®Ø±ÙˆØ¬ÛŒ Ø§ÛŒÙ† Ù…Ø¯Ù„ : 32000   \n",
        "\n",
        "Ø¢Ø®Ø±ÛŒÙ† Ù„Ø§ÛŒÙ‡ Ø®Ø·ÛŒ Ø§ÛŒÙ† Ù…Ø¯Ù„ Ú©Ù‡ Ø¨Ø±Ø¯Ø§Ø± Ù‡Ø§ÛŒ Ø®Ø±ÙˆØ¬ÛŒ ØªÙˆÙ„ÛŒØ¯ Ù…ÛŒÚ©Ù†Ø¯ :    \n",
        "\n",
        "ğŸŸ§ **(lm_loss): Linear(in_features=768, out_features=32000, bias=True)**\n"
      ],
      "metadata": {
        "id": "LK44vYnKLtA2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. CTRL**"
      ],
      "metadata": {
        "id": "IPkjz38eOaQx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "model_name = \"Salesforce/ctrl\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name,force_download=True)"
      ],
      "metadata": {
        "id": "tkKJKj1NOaQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = \"a little story\"\n",
        "input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "output = model.generate(input_ids, max_length=50, num_return_sequences=1)\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"\\n\\ngenerated_text : \", generated_text)"
      ],
      "metadata": {
        "id": "K6oJT7OKOaQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "jJS-mLMQOaQz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}