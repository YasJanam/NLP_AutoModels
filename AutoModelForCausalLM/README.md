### **CausalLM-Models** ( ูุฏู ูุง ุนู)
ุงู ููุน ูุฏู ูุง ุจุฑ ุงุณุงุณ ูุนูุงุฑ ุชุฑูุณููุฑูุฑ ูุง ุณุงุฎุชู ุดุฏู ุงูุฏ. ุงู ูุฏู ูุง ุจุฑุง ุชุณฺฉ ูุง ุฒุจุงู ุนู ุทุฑุงุญ ุดุฏู ุงูุฏุ ุนู __ูพุด ุจู ฺฉููู ุจุนุฏ ุฏุฑ ฺฉ ุชูุงู ุจุฑ ุงุณุงุณ ฺฉููุงุช ูุจู__

ูุซุงู ุงุฒ ฺฉ ูุฏู ุนู ุฏุฑ ููฺฉ ุฒุฑ ูพุงุฏู ุณุงุฒ ุดุฏู :

https://github.com/YasJanam/NLP_MODELS_1/tree/main/10_layer_nlp_model_3

#### CausalLM-Models Architecture (ูุนูุงุฑ ูุฏู ูุง ุนู)

 ูุฏู ูุง ุนู ุจุง ุฏฺฉูุฏุฑ ุชุฑุงูุณููุฑูุฑ ุณุงุฎุชู ูุดููุฏ

 ุฏฺฉูุฏุฑ ุชุฑุงูุณููุฑูุฑ ุดุงูู ฺูุฏู ูุงู ุงุฒ ุจูุงฺฉ ูุง ุฏฺฉูุฏุฑ ุงุณุช

 ูุฑ ุจูุงฺฉ ุฏฺฉูุฏุฑ ุดุงูู ุฏู ุจุฎุด ุงุตู ุงุณุช:
 ##### 1. ๐ Causal Self-Attention ๐
 -  ุฏุฑ ูุฏู ูุง ุนู ูฺฉุงูุฒู self-attention ุจู ุตูุฑุช ุนู ูพุงุฏู ุณุงุฒ ูุดูุฏ:
 
   - ุนู ูุฑ ฺฉููู ููุท ุจู ฺฉููุงุช __ูุจู__ ุชูุฌู ูฺฉูุฏุ ูู ุจู ฺฉููุงุช ุจุนุฏ

   - ุงู ฺฉุงุฑ ุจุง ุงุณุชูุงุฏู ุงุฒ ูุงุณฺฉ ุชูุฌู ( attention-mask ) ุงูุฌุงู ูุดูุฏ ฺฉู ฺฉููุงุช ุจุนุฏ ุฑุง ุจุฑุง ฺฉููู ูุนู ูุงูุฑุฆ ูฺฉูุฏ

##### 2. Feed-Forward Network

#### Causal Attention Mask (ูุงุณฺฉ ุชูุฌู ุนู )

ุฏุฑ ูุฏู ูุง ุนูุ ูุงุณฺฉ ุชูุฌู ุงุทููุงู ุญุงุตู ูฺฉูุฏ ฺฉู ูุฑ ฺฉููู __ููุท ุจู ฺฉููุงุช ูุจู__ ุชูุฌู ฺฉูุฏ.ุงู ูุงุณฺฉ ุจู ุตูุฑุช ฺฉ ูุงุชุฑุณ ุฏู ุจุนุฏ ูพุงุฏู ุณุงุฒ ูุดูุฏ ฺฉู ุฏุฑ ุขู ููุงุฏุฑ ุตูุฑ ูุดุงู ุฏููุฏู ุนุฏู ุชูุฌู ูุณุชูุฏ.

#### ูพุด ุจู ฺฉููู ุจุนุฏ 
ูุฏู ูุง causal  ฺฉ ูุงู ุฎุท ( Linear Layer ) ุฏุฑ ุงูุชูุง ุฏฺฉูุฏุฑ ุฏุงุฑูุฏ ฺฉู ุฎุฑูุฌ ุขู ฺฉ ุชูุฒุน ุงุญุชูุงู ุฑู ูุงฺฺฏุงู ( vocabulary ) ุงุณุช. ุงู ุชูุฒุน ุงุญุชูุงูุ ุจุฑุง ูพุด ุจู ฺฉููู ุจุนุฏ ุงุณุชูุงุฏู ูุดูุฏ.

#### ุขููุฒุด ูุฏู 
ูุฏู ูุง causalLM ุจุง ุงุณุชูุงุฏู ุงุฒ ุชุงุจุน ุฒุงู ุนู ุขููุฒุด ูุจููุฏ. ุงู ุชุงุจุน ุฒุงูุ ูุนูููุง __ุฒุงู ุงูุชุฑููพ ูุชูุงุทุน__ ุงุณุช (cross-entropy loss) ฺฉู ุจู ฺฉููู ูพุด ุจู ุดุฏู ู  ฺฉููู ูุงูุน ูุญุงุณุจู ูุดูุฏ.

#### ุชููุฏ ูุชู
ุฏุฑ ูุฑุญูู ุชููุฏ ูุชูุ ูุฏู ูุง causalLM ุจู ุตูุฑุช ุฎูุฏุฑฺฏุฑุณู ( autoregressive ) ูุชู ุชููุฏ ูฺฉููุฏ. ุนู __ฺฉููู ุจู ฺฉููู__ ูุชู ุฑุง ุชููุฏ ูฺฉููุฏุ ุจู ุทูุฑ ฺฉู ูุฑ ฺฉููู ุฌุฏุฏ ุจุฑ ุงุณุงุณ ฺฉููุงุช ูุจู ูพุด ุจู ูุดูุฏ.( ุจู ุดฺฉู ุนู )

#### ูุซุงู ูุนูุงุฑ GPT
ูุฏู ูุง GPT ุงุฒ ูุนูุงุฑ ุฏฺฉูุฏุฑ ุชุฑูุณููุฑูุฑ ุงุณุชูุงุฏู ูฺฉููุฏ. ูุนูุงุฑ GPT ุดุงูู ููุงุฑุฏ ุฒุฑ ุงุณุช : 
- Causal self-attention
- feed-forward network
- Layer Normalization
- Residual Connections ( https://gist.github.com/YasJanam/9a56f3efe299afe719e65f9c85bf20bf )

#### ูพุงุฑุงูุชุฑ ูุง ฺฉูุฏ
- Number of Layers ( ุชุนุฏุงุฏ ูุงู ูุง ) :
  
  - ุชุนุฏุงุฏ ุจูุงฺฉ ูุง ุฏฺฉูุฏุฑ ุฏุฑ ูุฏู
    
- Hidden Size :
  
  - ุชุนุฏุงุฏ ูุงุญุฏ ูุง ุฏุฑ ูุฑ ูุงู
    
- Number of Attention Heads :
  
  - ุชุนุฏุงุฏ ุณุฑ ูุง ููุงุฒ ุฏุฑ ูฺฉุงูุฒู ุชูุฌู
    
- Vocabulary Size :
  
  - ุชุนุฏุงุฏ ฺฉููุงุช ุง ุชูฺฉู ูุง ฺฉู ูุฏู ูุชูุงูุฏ ูพุด ุจู ฺฉูุฏ


    
  

