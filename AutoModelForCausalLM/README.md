### **CausalLM-Models** ( مدل های علی)
این نوع مدل ها بر اساس معماری ترنسفورمر ها ساخته شده اند. این مدل ها برای تسک های زبانی علی طراحی شده اند، یعنی __پیش بینی کلمه بعدی در یک توالی بر اساس کلمات قبلی__

مثالی از یک مدل علی در لینک زیر پیاده سازی شده :

https://github.com/YasJanam/NLP_MODELS_1/tree/main/10_layer_nlp_model_3

#### CausalLM-Models Architecture (معماری مدل های علی)

 مدل های علی با دیکودر ترانسفورمر ساخته میشوند

 دیکودر ترانسفورمر شامل چندین لایه از بلاک های دیکودر است

 هر بلاک دیکودر شامل دو بخش اصلی است:
 ##### 1. 🌟 Causal Self-Attention 🌟
 -  در مدل های علی مکانیزم self-attention به صورت علی پیاده سازی میشود:
 
   - یعنی هر کلمه فقط به کلمات __قبلی__ توجه میکند، نه به کلمات بعدی

   - این کار با استفاده از ماسک توجه ( attention-mask ) انجام میشود که کلمات بعدی را برای کلمه فعلی نامرئی میکند

##### 2. Feed-Forward Network

#### Causal Attention Mask (ماسک توجه علی )

در مدل های علی، ماسک توجه اطمینان حاصل میکند که هر کلمه __فقط به کلمات قبلی__ توجه کند.این ماسک به صورت یک ماتریس دو بعدی پیاده سازی میشود که در آن مقادیر صفر نشان دهنده عدم توجه هستند.

#### پیش بینی کلمه بعدی 
مدل های causal  یک لایه خطی ( Linear Layer ) در انتهای دیکودر دارند که خروجی آن یک توزیع احتمال روی واژگان ( vocabulary ) است. این توزیع احتمال، برای پیش بینی کلمه بعدی استفاده میشود.

#### آموزش مدل 
مدل های causalLM با استفاده از تابع زیان علی آموزش میبینند. این تابع زیان، معمولا __زیان انتروپی متقاطع__ است (cross-entropy loss) که بین کلمه پیش بینی شده و  کلمه واقعی محاسبه میشود.

#### تولید متن
در مرحله تولید متن، مدل های causalLM به صورت خودرگرسیو ( autoregressive ) متن تولید میکنند. یعنی __کلمه به کلمه__ متن را تولید میکنند، به طوری که هر کلمه جدید بر اساس کلمات قبلی پیش بینی میشود.( به شکل علی )

#### مثال معماری GPT
مدل های GPT از معماری دیکودر ترنسفورمر استفاده میکنند. معماری GPT شامل موارد زیر است : 
- Causal self-attention
- feed-forward network
- Layer Normalization
- Residual Connections ( https://gist.github.com/YasJanam/9a56f3efe299afe719e65f9c85bf20bf )

#### پارامتر های کلیدی
- Number of Layers ( تعداد لایه ها ) :
  
  - تعداد بلاک های دیکودر در مدل
    
- Hidden Size :
  
  - تعداد واحد ها در هر لایه
    
- Number of Attention Heads :
  
  - تعداد سر های موازی در مکانیزم توجه
    
- Vocabulary Size :
  
  - تعداد کلمات یا توکن هایی که مدل میتواند پیش بینی کند


    
  

